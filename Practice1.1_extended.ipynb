{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e53c2e",
   "metadata": {},
   "source": [
    "# Practice 1.1: Classical Neural Networks\n",
    "MQIST 2025/26: Quantum Computing and Machine Learning\n",
    "Alfredo Chavert Sancho\n",
    "Pedro Herrero Maldonado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e183b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Package Imports'\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras \n",
    "from keras import layers\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd1047",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295016b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(axis, train, validation, title):\n",
    "    # We create a list of epoch numbers from 1 to the length of the training set\n",
    "    epochs = range(1, len(train) + 1)\n",
    "    # Graph of the training data with a solid blue line\n",
    "    axis.plot(epochs, train, 'b-o', label='Training ' + title)\n",
    "    # Graph of the validation data with a red dashed line\n",
    "    axis.plot(epochs, validation, 'r--o', label='Validation '+ title)\n",
    "    # We set the title of the graph, the X and Y axis labels\n",
    "    axis.set_title('Training and validation ' + title)\n",
    "    axis.set_xlabel('Epochs')\n",
    "    axis.set_ylabel(title)\n",
    "    # We show the legend of the graph\n",
    "    axis.legend()    \n",
    "\n",
    "def multiplot(history):\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(11)\n",
    "    plot(axes[0], history.history['loss'], history.history['val_loss'], 'loss')\n",
    "    plot(axes[1], history.history['accuracy'], history.history['val_accuracy'], 'accuracy')\n",
    "\n",
    "    # We show the graphs on screen\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c475fb",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ce6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Load the Fashion-MNIST dataset'\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c0634",
   "metadata": {},
   "source": [
    "### Flatten the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4845aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28*28))\n",
    "test_images = test_images.reshape((10000, 28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569faac3",
   "metadata": {},
   "source": [
    "### Converting [0,255] integers to [0,1] floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57392f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf361d92",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6117f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "# test_labels = to_categorical(test_labels) # Not needed for categorical_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0ebd6",
   "metadata": {},
   "source": [
    "### Divide the train data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0580093",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = train_images[:10000]\n",
    "val_labels = train_labels[:10000]\n",
    "train_images = train_images[10000:]\n",
    "train_labels = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0b895",
   "metadata": {},
   "source": [
    "# Classical Neural Network Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b686c",
   "metadata": {},
   "source": [
    "In this section, we define the architecture of our classical neural network model. The model consists of an input layer, various hidden layers with activation functions, and an output layer with softmax activation for multi-class classification. The main goal of this section, in terms of performance, is to achieve the highest possible accuracy on the validation dataset even if the model overfits the training data.\n",
    "\n",
    "This issue will be later addressed in the next part of the practice, where we will implement regularization techniques to improve the model's generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae53a81",
   "metadata": {},
   "source": [
    "### Structural hyperparameters\n",
    "We will use the following structural hyperparameters for our classical neural network:\n",
    "- Number of hidden layers: \n",
    "- Number of neurons per hidden layer: \n",
    "- Activation function: \n",
    "\n",
    "The justification for these choices is as follows:\n",
    "- **Number of hidden layers**: \n",
    "- **Number of neurons per hidden layer**: \n",
    "- **Activation function**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfaeb8",
   "metadata": {},
   "source": [
    "### Learning hyperparameters\n",
    "We will use the following learning hyperparameters for our classical neural network:\n",
    "- Learning rate:\n",
    "- Batch size:\n",
    "- Number of epochs:\n",
    "- Optimizer:\n",
    "- Loss function:\n",
    "\n",
    "The justification for these choices is as follows:\n",
    "- **Learning rate**: A learning rate of 0.001 allows for gradual convergence to the optimal weights without overshooting.\n",
    "- **Batch size**: A batch size of 64 balances memory efficiency and gradient estimation accuracy.\n",
    "- **Number of epochs**: Training for 20 epochs provides enough iterations for the model to learn from the data without overfitting.\n",
    "- **Optimizer**: The Adam optimizer is chosen for its adaptive learning rate capabilities, which help in faster convergence.\n",
    "- **Loss function**: Categorical crossentropy is appropriate for multi-class classification tasks like Fashion-MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2534bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name='fashion_mnist')\n",
    "# Input layer\n",
    "model.add(layers.Input(shape=(28*28, )))\n",
    "# Hidden layers\n",
    "model.add(layers.Dense(50, name='hidden_1', activation='sigmoid')) # To change\n",
    "model.add(layers.Dense(50, name='hidden_2', activation='sigmoid')) # To change\n",
    "# Output layer\n",
    "model.add(layers.Dense(10, name='output', activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ed2e4",
   "metadata": {},
   "source": [
    "The learning hyperparameters are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54070811",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "opt = keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "loss_function = \"categorical_crossentropy\"\n",
    "model.compile(optimizer = opt, loss = loss_function, metrics = [\"accuracy\"])\n",
    "epochs = 25\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042fb79",
   "metadata": {},
   "source": [
    "Now the neural network is ready to be trained using the specified learning hyperparameters and tested with validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be421624",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data = (val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c1e88",
   "metadata": {},
   "source": [
    "We now plot the training and validation accuracy and loss curves to evaluate the model's performance over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889642ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "multiplot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd875a",
   "metadata": {},
   "source": [
    "*Put your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1765dd1",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model on the test dataset to determine its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Evaluate the model\n",
    "predicted_values = model.predict(test_images)\n",
    "predicted_classes = np.argmax(predicted_values, axis =1)\n",
    "# Classification report for precision, recall and f1-score\n",
    "report = classification_report(test_labels, predicted_classes, target_names = ['T-shirt/top', \n",
    "                                                                               'Trouser', \n",
    "                                                                               'Pullover', \n",
    "                                                                               'Dress', \n",
    "                                                                               'Coat', \n",
    "                                                                               'Sandal', \n",
    "                                                                               'Shirt', \n",
    "                                                                               'Sneaker', \n",
    "                                                                               'Bag', \n",
    "                                                                               'Ankle boot'])\n",
    "# Print the evaluation metrics\n",
    "print (\"Classification Report:\")\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e20b0d",
   "metadata": {},
   "source": [
    "## Regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085432ca",
   "metadata": {},
   "source": [
    "In this part of the practice, we will implement regularization techniques to improve the generalization capabilities of our classical neural network model. Regularization helps prevent overfitting by adding constraints to the model during training. Some common regularization techniques include dropout, L1/L2 regularization, and early stopping. We will experiment with these techniques to find the best combination that enhances the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885aeb58",
   "metadata": {},
   "source": [
    "### Early Stopping Implementation\n",
    "\n",
    "The early stopping mechanism is implemented to monitor the validation loss during training. If the validation loss does not improve for a specified number of consecutive epochs (patience), the training process is halted to prevent overfitting. This technique helps in maintaining the model's ability to generalize well on unseen data by stopping the training at the optimal point before overfitting occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c9e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c57e4057",
   "metadata": {},
   "source": [
    "### Dropout, batch normalization, weight regularization and initialization\n",
    "Some other regularization techniques that can be implemented are dropout, batch normalization, weight regularization and initialization.\n",
    "- **Dropout**: Randomly drops a fraction of neurons during training to prevent co-adaptation of neurons.\n",
    "- **Batch Normalization**: Normalizes the inputs of each layer to stabilize learning and improve convergence.\n",
    "- **Weight Regularization**: Adds a penalty to the loss function based on the magnitude of the weights (L1 or L2 regularization).\n",
    "- **Weight Initialization**: Proper initialization of weights can help in faster convergence and better performance and can be added to the model to further enhance its generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a37f75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7cd5010",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89706c1c",
   "metadata": {},
   "source": [
    "Comment on the results obtained by each model.\n",
    "- Make a reasoned comparison of the results obtained, where they have improved,\n",
    "worsened, etc.\n",
    "- Comment advantages, disadvantages of the different methods, conclusions\n",
    "obtained and other aspects of interest.\n",
    "- It is advisable to include a final graph or table summarizing all the results\n",
    "obtained by the different models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
